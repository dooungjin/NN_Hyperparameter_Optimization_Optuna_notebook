{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna as op\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import import_ipynb\n",
    "from kafnets import KAF\n",
    "\n",
    "# path to dataset\n",
    "data_path = '/domino/datasets/local/NN_Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features for NN\n",
    "old_fea_csv = pd.read_csv(data_path + 'feature_list.csv')\n",
    "old_fea_list = old_fea_csv['Variable Name'].to_list()\n",
    "old_fea_list_set = set(old_fea_list)\n",
    "print(\"Number of features:\", len(old_fea_list))\n",
    "\n",
    "fea_csv = pd.read_csv(data_path + 'new_feature_list.csv')\n",
    "fea_list = fea_csv['Variable Name'].to_list()\n",
    "fea_list_set = set(fea_list)\n",
    "print(\"Number of features:\", len(fea_list))\n",
    "\n",
    "diff = fea_list_set.difference(old_fea_list_set)\n",
    "print(\"Number of replaced features:\", len(diff))\n",
    "\n",
    "# training data\n",
    "p_in_time_train = pd.read_csv(data_path + 'training_set.csv')\n",
    "\n",
    "# validation data\n",
    "p_in_time_valid = pd.read_csv(data_path + 'validation_set.csv')\n",
    "\n",
    "# mean, std of training data\n",
    "training_mean, training_std = p_in_time_train[fea_list].mean(), p_in_time_train[fea_list].std()\n",
    "\n",
    "# Imputation\n",
    "p_in_time_train[fea_list] = p_in_time_train[fea_list].fillna(training_mean)\n",
    "p_in_time_valid[fea_list] = p_in_time_valid[fea_list].fillna(training_mean)\n",
    "\n",
    "# Training and Validation data for NN\n",
    "X_train, y_train = p_in_time_train[fea_list], 1 - p_in_time_train[\"label\"]\n",
    "X_valid, y_valid = p_in_time_valid[fea_list], 1 - p_in_time_valid[\"label\"]\n",
    "\n",
    "# Standardization\n",
    "X_train_std = (X_train - training_mean)/training_std\n",
    "X_valid_std = (X_valid - training_mean)/training_std\n",
    "X_train_std.shape, y_train.shape, X_valid_std.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NNdataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x, dtype = torch.float32)\n",
    "        self.y = torch.tensor(y, dtype = torch.float32)\n",
    "        self.length = self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "train_dataset = NNdataset(X_train_std.values, y_train.values)\n",
    "valid_dataset = NNdataset(X_valid_std.values, y_valid.values)\n",
    "\n",
    "train_dataloader = DataLoader(dataset = train_dataset, shuffle = True, batch_size = 512)\n",
    "valid_dataloader = DataLoader(dataset = valid_dataset, shuffle = False, batch_size = 512)\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_shape, dropout_rate, dim1, dim2, act1, act2):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape, dim1)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        self.BN1 = nn.BatchNorm1d(dim1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(dim1, dim2)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        self.BN2 = nn.BatchNorm1d(dim2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(dim2, 1)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        if act1 == \"ReLU\":\n",
    "            self.act1 = nn.ReLU()\n",
    "        if act1 == \"ELU\":\n",
    "            self.act1 = nn.ELU()\n",
    "        if act1 == \"LeakyReLU\":\n",
    "            self.act1 = nn.LeakyReLU()\n",
    "        if act1 == \"ReLU6\":\n",
    "            self.act1 = nn.ReLU6()\n",
    "            \n",
    "        if act2 == \"ReLU\":\n",
    "            self.act2 = nn.ReLU()\n",
    "        if act2 == \"ELU\":\n",
    "            self.act2 = nn.ELU()\n",
    "        if act2 == \"LeakyReLU\":\n",
    "            self.act2 = nn.LeakyReLU()\n",
    "        if act2 == \"ReLU6\":\n",
    "            self.act2 = nn.ReLU6()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.act1(self.BN1(self.fc1(x))))\n",
    "        x = self.dropout(self.act2(self.BN2(self.fc2(x))))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, optimizer, criterion):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    epochs = 200\n",
    "    patience = 10\n",
    "    model.to(device)\n",
    "    \n",
    "    best_auc, num_patience = 0, 0\n",
    "    train_loss, valid_loss = [], []\n",
    "    train_aucs, valid_aucs = [], []\n",
    "    for epoch in range(epochs):\n",
    "        train_epoch_loss, valid_epoch_loss = 0, 0\n",
    "        train_score, train_label = np.zeros(len(y_train)), np.zeros(len(y_train))\n",
    "        valid_score, valid_label = np.zeros(len(y_valid)), np.zeros(len(y_valid))\n",
    "\n",
    "        model.train()\n",
    "        idx = 0\n",
    "        for i, (train_x, train_y) in enumerate(train_dataloader):\n",
    "            y_pred = model(train_x.to(device))\n",
    "            train_score[idx:idx + len(y_pred)] = np.squeeze(y_pred.cpu().detach().numpy(), 1)\n",
    "            train_label[idx:idx + len(y_pred)] = train_y\n",
    "            idx += len(y_pred)\n",
    "\n",
    "            loss = criterion(y_pred, train_y.reshape(-1, 1).to(device))\n",
    "            train_epoch_loss += loss.cpu().detach().numpy()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        idx = 0\n",
    "        for i, (valid_x, valid_y) in enumerate(valid_dataloader):\n",
    "            y_pred = model(valid_x.to(device))\n",
    "            valid_score[idx:idx + len(y_pred)] = np.squeeze(y_pred.cpu().detach().numpy(), 1)\n",
    "            valid_label[idx:idx + len(y_pred)] = valid_y\n",
    "            idx += len(y_pred)\n",
    "\n",
    "            loss = criterion(y_pred, valid_y.reshape(-1, 1).to(device))\n",
    "            valid_epoch_loss += loss.cpu().detach().numpy()\n",
    "\n",
    "        train_auc = roc_auc_score(train_label, train_score)\n",
    "        valid_auc = roc_auc_score(valid_label, valid_score)\n",
    "\n",
    "        if (valid_auc < best_auc) and (num_patience < patience): # need patience\n",
    "            num_patience += 1\n",
    "\n",
    "        elif (valid_auc < best_auc) and (num_patience == patience): \n",
    "            break\n",
    "\n",
    "        else:\n",
    "            best_auc = valid_auc\n",
    "    return best_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):    \n",
    "    dim1 = trial.suggest_int('dim1', 32, 256, 32)\n",
    "    dim2 = trial.suggest_int('dim2', 32, 256, 32)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.1, log = False)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5, log = False)\n",
    "    \n",
    "    act1 = trial.suggest_categorical('act1', ['ReLU', 'ELU', 'LeakyReLU', 'ReLU6'])\n",
    "    act2 = trial.suggest_categorical('act2', ['ReLU', 'ELU', 'LeakyReLU', 'ReLU6'])\n",
    "\n",
    "    model = Net(input_shape = len(fea_list), dropout_rate = dropout_rate, dim1 = dim1, dim2 = dim2, act1 = act1, act2 = act2)\n",
    "    \n",
    "    selected_optimizer = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "    if selected_optimizer == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.0)\n",
    "    \n",
    "    criterion  = nn.BCELoss()\n",
    "    best_auc = training(model, optimizer, criterion)\n",
    "            \n",
    "    return best_auc\n",
    "\n",
    "study = op.create_study(direction = \"maximize\")\n",
    "study.optimize(objective, n_trials = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
